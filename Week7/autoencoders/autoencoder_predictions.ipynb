{"cells":[{"cell_type":"markdown","metadata":{"id":"gVtwtUTqf2Is"},"source":["### Autoencoders \n","Autoencoders are a data-driven network-trained dimentionality reduction technique.  The network encodes the input image and then decodes using random weights (at initialization). The decoded image is compared with the input image to adjust the weights of the encoder-decoder. \n","\n","In principle, the encoder-decoder architecture reduces the dimentionality of the input data and tries to maintain only the most relevant information. This information is available in the bottleneck layer of the encoder-decoder architecture which is also called latent-space representation. \n","\n","An image of the top-level architecture is shown below: \n","\n","\n","![picture](https://d3i71xaburhd42.cloudfront.net/a81895c377ba4789997a1ac3a870c0098ba5a2d5/1-Figure1-1.png)\n","\n","\n","### Some Applications: \n","\n","The following are some of the applications of Autoencoders: \n","\n","1) *Dimentionality Reduction* - its easier to understand the relationship between two variables instead of 1000 variables on each other. â˜¹\n","\n","2) *Anomaly Detection* - the bottleneck layer provides helpful information about the relationship. So for example if some images have noise in it and most of the data is noise-free. Then the encoder will not encode *this noise* into the latent space representation (**hopefully**) and we can compare the original and recontstucted images and substact the noise. \n","\n","3) *Information Retrieval*\n","\n","4) *Image Segmantation* \n"]},{"cell_type":"code","execution_count":110,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1610,"status":"ok","timestamp":1647951884721,"user":{"displayName":"Muhammad Arshad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvFtQPVWmKWzoK5CKIhL-D8VwVuWRpqJFIdPiupQ=s64","userId":"04461893466741361594"},"user_tz":0},"id":"sZcE0ATjUx5j","outputId":"9927b883-4bc3-4511-de6b-e832e0c82add"},"outputs":[{"name":"stdout","output_type":"stream","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":111,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1647951884721,"user":{"displayName":"Muhammad Arshad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvFtQPVWmKWzoK5CKIhL-D8VwVuWRpqJFIdPiupQ=s64","userId":"04461893466741361594"},"user_tz":0},"id":"yH5d0EDeT5mr"},"outputs":[],"source":["# imports\n","from typing import Type\n","import os\n","import json\n","from datetime import datetime\n","import numpy as np\n","import tensorflow as tf\n","from skimage.io import imread\n","from skimage.transform import resize\n","import imageio\n"]},{"cell_type":"code","execution_count":112,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1647951884721,"user":{"displayName":"Muhammad Arshad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvFtQPVWmKWzoK5CKIhL-D8VwVuWRpqJFIdPiupQ=s64","userId":"04461893466741361594"},"user_tz":0},"id":"f4wWDkUoUlGZ"},"outputs":[],"source":["# const\n","MODEL_DIR = \"/content/drive/MyDrive/Colab Notebooks/Week7/autoencoders/model/\"\n","MODEL_NAME = \"rtp-rgbd\"\n","IMAGE_DIR = \"/content/drive/MyDrive/Colab Notebooks/Week7/data/rtp-rgb/color_img\"\n","EXP_DIR = \"/content/drive/MyDrive/Colab Notebooks/Week7/autoencoders\"\n","IMAGE_RESHAPE =  (256, 256, 3)\n","ID_TEST = [\"001\", \"002\", \"003\", \"004\", \"005\", \"006\", \"007\", \"008\", \"031\"]\n"]},{"cell_type":"code","execution_count":113,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1647951884721,"user":{"displayName":"Muhammad Arshad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvFtQPVWmKWzoK5CKIhL-D8VwVuWRpqJFIdPiupQ=s64","userId":"04461893466741361594"},"user_tz":0},"id":"qayErM2FWXd7"},"outputs":[],"source":["def load_image_batch(img_dir, id_list, img_ext = \".png\", resize_shape = None) -> np.ndarray:\n","    \"\"\"Load a batch of images from a directory.\n","\n","    Args:\n","        img_dir (str): Path of the directory.\n","        id_list (list[str]): List of image ids.\n","        img_ext (str, optional): Extension of the image files. Defaults to \".png\".\n","        resize_shape (tuple[int], optional): If specified, reshape all images to this shape. Defaults to None.\n","\n","    Returns:\n","        np.ndarray: The image data with shape (samples, width, height, channels).\n","    \"\"\"\n","\n","    image_data = []\n","    for id in id_list:\n","        img = load_image(os.path.join(img_dir, id + img_ext), resize_shape)\n","        image_data.append(img)\n","\n","    return np.array(image_data)\n","\n","def load_image(img_path, resize_shape = None) -> np.ndarray:\n","    \"\"\"Load an image from file.\n","\n","    Args:\n","        img_path (str): Path of the image.\n","        resize_shape (tuple[int], optional): If specified, reshape all images to this shape. Defaults to None.\n","\n","    Returns:\n","        np.ndarray: The image data.\n","    \"\"\"\n","\n","    img = imread(img_path, pilmode='RGB')\n","    if resize_shape:\n","        img = resize(img, resize_shape)\n","    return img\n"]},{"cell_type":"code","execution_count":114,"metadata":{"executionInfo":{"elapsed":7,"status":"ok","timestamp":1647951884722,"user":{"displayName":"Muhammad Arshad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvFtQPVWmKWzoK5CKIhL-D8VwVuWRpqJFIdPiupQ=s64","userId":"04461893466741361594"},"user_tz":0},"id":"bBJsJCFdYvsB"},"outputs":[],"source":["def evaluate_batch(encoder, autoencoder, X_test, name):\n","  \"\"\"Evaluate a batch of data.\n","\n","  Args:\n","      model (tf.keras.Model): The model to evaluate.\n","      X_test (np.ndarray): Input data.\n","      y_test (np.ndarray): Output data.\n","      name (str): Name of the run.\n","\n","  Returns:\n","      dict[str, float]: Dictionary containing metric names and values.\n","  \"\"\"\n","\n","  # Create necessary folders.\n","  BOTTLENECK_DIR = os.path.join(EXP_DIR, \"output\", name, 'bottleneck')\n","  RECONSTRUCTED_DIR = os.path.join(EXP_DIR, \"output\", name, 'reconstructed')\n","  ensure_dirs([BOTTLENECK_DIR, RECONSTRUCTED_DIR])\n","\n","  # Evaluation.\n","  reconstructed_images = autoencoder.predict(X_test)\n","  bottleneck_images = encoder.predict(X_test)\n","\n","  for i, (img_orig, img_bott, img_rec) in enumerate(zip(X_test, bottleneck_images, reconstructed_images)):\n","      imageio.imwrite(os.path.join(BOTTLENECK_DIR, f\"encoded_{i+1}.jpg\"), (img_bott * 255).astype(np.uint8))\n","      img_comparison = np.hstack((img_orig, img_rec))\n","      imageio.imwrite(os.path.join(RECONSTRUCTED_DIR, f\"reconstructed_{i+1}.jpg\"), (img_comparison * 255).astype(np.uint8))\n","\n","  # No metrics.\n","  return {}\n","  \n","def ensure_dirs(dir_paths):\n","  \"\"\"Create these directories if they do not exist yet.\n","\n","  Args:\n","      dir_paths (list[str]): List of directory paths to create.\n","  \"\"\"\n","  for dir_path in dir_paths:\n","      os.makedirs(dir_path, exist_ok=True)\n"]},{"cell_type":"code","execution_count":115,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1647951884722,"user":{"displayName":"Muhammad Arshad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvFtQPVWmKWzoK5CKIhL-D8VwVuWRpqJFIdPiupQ=s64","userId":"04461893466741361594"},"user_tz":0},"id":"wXP9dRtqV9b_"},"outputs":[],"source":["def evaluate():\n","  # Load model.\n","  autoencoder = tf.keras.models.load_model(os.path.join(MODEL_DIR, MODEL_NAME))\n","  encoder = tf.keras.Model(autoencoder.get_layer(\"input_image\").output, autoencoder.get_layer(\"bottleneck\").output)\n","\n","  # Load data.\n","  input_img = load_image_batch(IMAGE_DIR, ID_TEST, resize_shape=IMAGE_RESHAPE)\n","\n","  # evaluate the model \n","  evaluate_batch(encoder, autoencoder, input_img, MODEL_NAME)\n"]},{"cell_type":"code","execution_count":116,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2648,"status":"ok","timestamp":1647951887364,"user":{"displayName":"Muhammad Arshad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvFtQPVWmKWzoK5CKIhL-D8VwVuWRpqJFIdPiupQ=s64","userId":"04461893466741361594"},"user_tz":0},"id":"HnDZ-g3pfCFY","outputId":"b26bba93-a01d-43e4-d9aa-b5a3273b4179"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n"]}],"source":["if __name__ == \"__main__\":\n","  evaluate()"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyN228AbwV7sBvhj67OcMf58","collapsed_sections":[],"name":"autoencoder_predictions.ipynb","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
