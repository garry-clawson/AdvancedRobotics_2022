{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"rtp_rgbd_cnn.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM8l70usjECvHEM6OTCJX8m"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_vsrZV_H1i68","executionInfo":{"status":"ok","timestamp":1648120140857,"user_tz":0,"elapsed":2295,"user":{"displayName":"Muhammad Arshad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvFtQPVWmKWzoK5CKIhL-D8VwVuWRpqJFIdPiupQ=s64","userId":"04461893466741361594"}},"outputId":"bac3b8f1-cf8a-43b7-9216-2c5e2f4b49aa"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}],"source":["# to access the google drive.\n","from google.colab import drive\n","drive.mount('/content/drive')\n"]},{"cell_type":"code","source":["# to import local python modules.\n","import sys\n","sys.path.insert(0,\"/content/drive/MyDrive/Colab-Notebooks/AdvanceRoboticsWorkshop/rtp_cnn/\")\n","from promp import ProMP"],"metadata":{"id":"ZuzT5M-x132F","executionInfo":{"status":"ok","timestamp":1648120140858,"user_tz":0,"elapsed":8,"user":{"displayName":"Muhammad Arshad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvFtQPVWmKWzoK5CKIhL-D8VwVuWRpqJFIdPiupQ=s64","userId":"04461893466741361594"}}},"execution_count":24,"outputs":[]},{"cell_type":"code","source":["# imports\n","import os\n","import json\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras import Model\n","import tensorflow.keras.layers as layers\n","from tensorflow.python.keras.metrics import MeanMetricWrapper\n","import abc\n","import matplotlib.pyplot as plt\n","from skimage.io import imread\n","from skimage.transform import resize\n","import random\n"],"metadata":{"id":"PFqKp2JDASBr","executionInfo":{"status":"ok","timestamp":1648120140859,"user_tz":0,"elapsed":9,"user":{"displayName":"Muhammad Arshad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvFtQPVWmKWzoK5CKIhL-D8VwVuWRpqJFIdPiupQ=s64","userId":"04461893466741361594"}}},"execution_count":25,"outputs":[]},{"cell_type":"code","source":["class RTP_RGBD:\n","  \"\"\" this class contains all the preprocessing, training and testing codes for\n","      RTB-RGB CNN model network.\n","\n","  \"\"\"\n","  def __init__(self):\n","\n","    # ProMP configurations\n","    self.n_basis = 10\n","    self.n_dof = 7\n","    self.n_t = 100\n","\n","    # Trajectories -> ProMP Weights -> Model Labels\n","    self.promp = ProMP(n_basis = self.n_basis, n_dof = self.n_dof, n_t = self.n_t )\n","\n","    # Encoder configurations.\n","    self.AUTOENCODER_MODEL_PATH = \"/content/drive/MyDrive/Colab-Notebooks/AdvanceRoboticsWorkshop/autoencoders/model/rtp-rgbd/\"\n","    self.IMAGE_DIR = \"/content/drive/MyDrive/Colab-Notebooks/AdvanceRoboticsWorkshop/data/rtp-rgbd/color_img/\"\n","    self.IMAGE_RESHAPE = (256, 256, 3)\n","    \n","    # Generate a simple model from trained autoencoder. \n","    # This model takes in input image and returns bottleneck-layer. \n","    # Input Image -> Encoder -> bottleneck output -> CNN\n","    model_layers = tf.keras.models.load_model(self.AUTOENCODER_MODEL_PATH)\n","    bottleneck_layer = model_layers.get_layer(\"bottleneck\").output\n","    self.encoder = Model(inputs=model_layers.inputs, outputs=bottleneck_layer, name=\"encoder\")\n","\n","    self.load_dataset()\n","\n","    # Model configuration \n","    self.LOSS_DIR = \"/content/drive/MyDrive/Colab-Notebooks/AdvanceRoboticsWorkshop/rtp_cnn/model/\"\n","    self.MODEL_DIR = \"/content/drive/MyDrive/Colab-Notebooks/AdvanceRoboticsWorkshop/rtp_cnn/model/\"\n","    self.BATCH_SIZE = 10\n","    self.EPOCHS = 6000\n","    self.LR = 1e-7\n","    self.TEST_SIZE = 10\n","    self.MODEL_NAME = \"rtp_rgbd_cnn_1\"\n","    self.MODEL_WEIGHTS_PATH = os.path.join(self.MODEL_DIR, self.MODEL_NAME, \"weights\")\n","\n","    self.load_model()\n","\n","  def load_dataset(self) -> None:\n","    '''Load data i.e. color images and joint_position from data.\n","    ''' \n","\n","    # Generate file ids based on regions. \n","    regions = [\"A\", \"B\", \"C\", \"D\"]\n","    total = 10\n","    id_list = [ region + \"_\" + str(n).zfill(3) for n in range(1,total) for region in regions]\n","    \n","    train_set = int (0.7 * len(id_list))\n","    test_set = len(id_list) - train_set\n","    id_list_train , id_list_test = random.sample( id_list, train_set ), random.sample( id_list, test_set )\n","    self.idx_list_train = [ id_list.index(i) for i in id_list_train if id_list.index(i) < len(id_list)]\n","    self.idx_list_test  = [id_list.index(i) for i in id_list_test if id_list.index(i) < len(id_list)]\n","\n","\n","    # load encoded images \n","    self.encoded_images = self.load_encoded_images(self.IMAGE_DIR, id_list, self.encoder, self.IMAGE_RESHAPE)\n","\n","    # Labels configuration.\n","    self.TRAJ_DIR = \"/content/drive/MyDrive/Colab-Notebooks/AdvanceRoboticsWorkshop/data/rtp-rgbd/trajectories/\"\n","\n","    # Load trajectories.\n","    trajectories = self.load_array_from_json(self.TRAJ_DIR, \"joint_position\", id_list, slicer=np.s_[..., 0:self.n_dof])\n","\n","    # Convert each trajectory into ProMP weights.\n","    self.promp_weights = np.zeros((len(trajectories), self.promp.n_basis * self.promp.n_dof))\n","    for i, trajectory in enumerate(trajectories):\n","        self.promp_weights[i, :] = self.promp.weights_from_trajectory(trajectory)\n","\n","\n","  def load_encoded_images(self, img_dir, id_list, encoder, resize_shape = None, normalize = False) -> np.ndarray:\n","    \"\"\"Load multiple images by ID from a directory an pass them through an encoder model.\n","\n","    Args:\n","        img_dir (str): Path of the directory.\n","        id_list (np.ndarray): List of image id\n","        encoder (tf.keras.Model): The encoder model.\n","        resize_shape (tuple[int], optional): If specified, reshape all images to this shape. Defaults to None.\n","        normalize (bool, optional): If True, normalize the image in [-1, 1]. Defaults to False.\n","\n","    Returns:\n","        np.ndarray: The image data with shape (samples, width, height, channels).\n","    \"\"\"\n","    return np.array([self.load_encoded_image(os.path.join(img_dir, id + \".png\"), encoder, resize_shape, normalize) for id in id_list])\n","  \n","  def load_encoded_image(self, img_path, encoder, resize_shape = None, normalize = False) -> np.ndarray:\n","      \"\"\"Load an image an pass it through an encoder model.\n","\n","      Args:\n","          img_path (str): Path of the image.\n","          encoder (tf.keras.Model): The encoder model.\n","          resize_shape (tuple[int], optional): If specified, reshape all images to this shape. Defaults to None.\n","          normalize (bool, optional): If True, normalize the image in [-1, 1]. Defaults to False.\n","\n","      Returns:\n","          np.ndarray: The image data with shape (width, height, channels).\n","      \"\"\"\n","      return np.squeeze(encoder(np.expand_dims(self.load_image(img_path, resize_shape, normalize), axis=0)))\n","\n","  def load_image(self, img_path, resize_shape = None, normalize = False) -> np.ndarray:\n","      \"\"\"Load an image from file.\n","\n","      Args:\n","          img_path (str): Path of the image.\n","          resize_shape (tuple[int], optional): If specified, reshape all images to this shape. Defaults to None.\n","          normalize (bool, optional): If True, normalize the image in [-1, 1]. Defaults to False.\n","\n","      Returns:\n","          np.ndarray: The image data.\n","      \"\"\"\n","\n","      img = imread(img_path, pilmode='RGB')\n","      if resize_shape:\n","          img = resize(img, resize_shape)\n","      if normalize:\n","          img = self.normalize_negative_one(img)\n","      return img\n","\n","  def normalize_negative_one(self, img, two_d=True):\n","      normalized_input = (img - np.amin(img)) / (np.amax(img) - np.amin(img))\n","      if two_d:\n","          return 2*normalized_input - 1\n","      else:\n","          return 2*normalized_input - 1, np.amin(img), np.amax(img)\n","\n","\n","  def load_array_from_json(self, dir_path, json_key, id_list, slicer = None):\n","      \"\"\"Load (a slice of) a Numpy array from multiple JSON files in a folder.\n","\n","      Can be used, for example, to load joint states from multiple trajectory files.\n","\n","      Args:\n","          dir_path (str): The path of the directory containing the files.\n","          json_keys (list[str]): The keys to be read from the JSON files.\n","          id_list (list[str]): The ids of the files to be read.\n","          slicer (tuple[slice], optional): If specified, slice every array accoroding to this slicer. Defaults to None.\n","\n","      Returns:\n","          list[np.ndarray]: An array for each key.\n","      \"\"\"\n","\n","      data = []\n","      for id in id_list:\n","          file_path = os.path.join(dir_path, f\"{id}.json\")\n","          with open(file_path, \"r\") as file:\n","              json_data = json.load(file)[json_key]\n","              arr = np.array(json_data)\n","              if slicer:\n","                  arr = arr[slicer]\n","              data.append(arr)\n","\n","      return data\n","\n","\n","\n","  def load_model(self, name = \"RTP\"):\n","\n","    self.optimizer = tf.keras.optimizers.Adam(learning_rate=self.LR)\n","    self.loss = self.get_joint_loss(self.promp)\n","    self.metrics_test = [\n","        RmseJointsPromp(self.promp, promp_space=\"joint\", name=\"rmse_joints\")\n","    ]\n","\n","    self.metrics_train = [\n","        RmseJointsPromp(self.promp, promp_space=\"joint\", name=\"rmse_joints\"),\n","    ]\n","\n","\n","    l1_reg = 0.0\n","    l2_reg = 0.0\n","    l1_l2_reg = tf.keras.regularizers.l1_l2(l1_reg, l2_reg)\n","\n","    input_layer = layers.Input(shape=(32, 32, 3), name=\"encoded_image_input\")\n","\n","    # convolution sub-network\n","    x = layers.Conv2D(32, (3, 3), padding=\"same\", kernel_regularizer=l1_l2_reg)(input_layer)\n","    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n","    x = layers.Dropout(0.25)(x)\n","\n","    x = layers.Conv2D(16, (3, 3), padding=\"same\", kernel_regularizer=l1_l2_reg)(x)\n","    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n","    x = layers.Dropout(0.25)(x)\n","\n","    x = layers.Conv2D(8, (3, 3), padding=\"same\", kernel_regularizer=l1_l2_reg)(x)\n","    x = layers.MaxPooling2D(pool_size=(2, 2))(x)\n","    x = layers.Dropout(0.25)(x)\n","\n","    x = layers.Conv2D(4, (3, 3), padding=\"same\", kernel_regularizer=l1_l2_reg)(x)\n","    x = layers.Flatten(name=\"feature_vec\")(x)\n","\n","    # fully connected sub-network\n","    neorons = [64]\n","    for dense_size in [64]:\n","        x = layers.Dense(dense_size, activation=\"relu\", kernel_regularizer=l1_l2_reg)(x)\n","        \n","    output_layer = layers.Dense(self.promp.n_dof * self.promp.n_basis, activation=\"linear\")(x)\n","    return Model(inputs=input_layer, outputs=output_layer, name=name)\n","\n","  def get_joint_loss(self, promp):\n","      \"\"\"Obtain the loss function evaluating the RMSE on the joint trajectories from the full ProMP weights (in TensorFlow).\n","\n","      Args:\n","          promp (ProMP): An initialized ProMP instance.\n","\n","      Returns:\n","          The loss function\n","      \"\"\"\n","\n","      def joint_loss(promp_true, promp_pred):\n","          \"\"\"RMSE loss on the joint trajectory from the full ProMP weights.\n","\n","          Args:\n","              promp_true: Ground truth full ProMP weights with shape (n_batch, n_basis * n_dof).\n","              promp_pred: Predicted full ProMP weights with shape (n_batch, n_basis * n_dof).\n","\n","          Returns:\n","              The resulting loss.\n","          \"\"\"\n","          traj_true = promp.trajectory_from_weights_tf(promp_true)\n","          traj_pred = promp.trajectory_from_weights_tf(promp_pred)\n","          # RMSE on joint trajectories.\n","          loss = tf.sqrt(tf.reduce_mean(tf.square(traj_true - traj_pred), axis=1))\n","          # Average over batches and joints.\n","          loss = tf.reduce_mean(loss)\n","          return loss\n","\n","      return joint_loss\n","      \n","\n","  def get_data(self, indices) :\n","      \"\"\"Get the input and output data for the model.\n","\n","      Args:\n","          indices (np.ndarray): The list of indices to slice the dataset.\n","\n","      Returns:\n","          tuple[np.ndarray, np.ndarray]: Input and output data for the model.\n","      \"\"\"\n","      X = self.encoded_images[indices]\n","      y = self.promp_weights[indices]\n","\n","      return X, y\n","\n","  def train(self):\n","\n","      model = self.load_model()\n","      model.summary(print_fn=print)\n","\n","      model.compile(optimizer=self.optimizer, loss=self.loss, metrics=self.metrics_train)\n","\n","      # Load the data.\n","      print (\"Encoded images size: \", self.encoded_images.shape)\n","\n","      X_train, y_train = self.get_data(self.idx_list_train)\n","      X_val, y_val = self.get_data(self.idx_list_test)\n","\n","      # Training.\n","      history = model.fit(\n","          X_train, y_train, validation_data=(X_val, y_val),\n","          epochs=self.EPOCHS, batch_size=self.BATCH_SIZE, shuffle=True, verbose=0\n","      )\n","\n","      model.save_weights(self.MODEL_WEIGHTS_PATH)\n","\n","      # Plot the loss history.\n","      self.plot_metric(\n","          [history.history[\"loss\"], history.history[\"val_loss\"]],\n","          [\"train\", \"val\"], ylabel=\"loss\",\n","          save_path=os.path.join(self.LOSS_DIR, \"loss_{}.png\".format(self.MODEL_NAME))\n","      )\n","\n","      # Save the actual history values.\n","      np.save(os.path.join(self.LOSS_DIR, 'train_history'), history.history)\n","\n","\n","  def plot_metric(\n","      self,\n","      data,\n","      legend_labels = None, title = None,\n","      ylabel = None, xlabel = \"epoch\",\n","      yscale = \"linear\",\n","      save_path = None, show = False\n","  ):\n","      \"\"\"Plot the history of a metric during a training episode.\n","\n","      Args:\n","          data: History of the metric values with shape (n_samples,) or (n_channels, n_samples).\n","          legend_labels (list[str], optional): Legend labels. Defaults to None.\n","          title (str, optional): Title of the graph. Defaults to None.\n","          ylabel (str, optional): Label of the y axis. Defatuls to None.\n","          xlabel (str, optional): Label of the x axis. Defaults to \"epoch\".\n","          yscale (str, optional): The y scale of the plot. Defaults to \"linear\".\n","          save_path (str, optional): If specified, the file where the plot image is to be saved. Defaults to None.\n","          show (bool, optional): If True, displays the plot in a window, halting execution. Defaults to False.\n","      \"\"\"\n","\n","      data = np.array(data)\n","      epochs = range(1, data.shape[-1]+1)\n","\n","      fig = plt.figure()\n","      plt.plot(epochs, np.transpose(data))\n","      plt.yscale(yscale)\n","      plt.grid(True, which='both')\n","      plt.title(title)\n","      plt.xlabel(xlabel)\n","      plt.ylabel(ylabel)\n","      if legend_labels:\n","          plt.legend(legend_labels)\n","\n","      if save_path:\n","          fig.savefig(save_path)\n","      if show:\n","          plt.show()\n","      plt.close(fig)"],"metadata":{"id":"yXlGK7l6AaYW","executionInfo":{"status":"ok","timestamp":1648120216425,"user_tz":0,"elapsed":1380,"user":{"displayName":"Muhammad Arshad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvFtQPVWmKWzoK5CKIhL-D8VwVuWRpqJFIdPiupQ=s64","userId":"04461893466741361594"}}},"execution_count":31,"outputs":[]},{"cell_type":"code","source":["class PrompTrajMeanMetric(abc.ABC, MeanMetricWrapper):\n","    \"\"\"Custom base class for a metric taking ProMP weights as input and computing\n","    the metric value on the corresponding trajectory.\n","\n","    The metric is averaged on all batches of each epoch.\n","    \"\"\"\n","\n","    def __init__(self, promp: ProMP, promp_space: str, name: str, **kwargs):\n","        \"\"\"\n","\n","        Args:\n","            promp (ProMP): A initialized ProMP instance to compute weights and trajectories.\n","            promp_space (str): The space where ProMP trajectories live. Must be either \"joint\" or \"task\".\n","            name (str): The name of the metric\n","        \"\"\"\n","        # MeanMetricWrapper is used to compute the mean on the batches.\n","        super().__init__(fn=self.metric_fn, name=name, **kwargs)\n","        # ProMP variables to compute the trajectory from the weights.\n","        self.promp = promp\n","        assert promp_space in [\"joint\", \"task\"], \"The 'promp_space' parameter must be either 'joint' or 'task'.\"\n","        self.promp_space = promp_space\n","\n","    @abc.abstractmethod\n","    def metric_fn(self, promp_true, promp_pred):\n","        \"\"\"Compute the metric value on a batch of data.\n","\n","        Args:\n","            promp_true: Ground truth tensor of the ProMP weights with shape (n_batch, n_dof*n_basis).\n","            promp_pred: Predicted tensor of the ProMP weights with shape (n_batch, n_dof*n_basis).\n","\n","        Returns:\n","            The value of the metric on this batch.\n","        \"\"\"\n","        pass\n","\n","\n","\n"],"metadata":{"id":"Iw0uLzHMLzds","executionInfo":{"status":"ok","timestamp":1648120141257,"user_tz":0,"elapsed":5,"user":{"displayName":"Muhammad Arshad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvFtQPVWmKWzoK5CKIhL-D8VwVuWRpqJFIdPiupQ=s64","userId":"04461893466741361594"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["class RmseJointsPromp(PrompTrajMeanMetric):\n","    \"\"\"RMSE of the joint trajectories from a batch of ProMP weights of joint trajectories.\n","\n","    The value is averaged on all batches of each epoch.\n","    \"\"\"\n","\n","    def __init__(self, promp: ProMP, promp_space: str, name: str = \"rmse_joints\", **kwargs):\n","        super().__init__(promp=promp, promp_space=promp_space, name=name, **kwargs)\n","\n","    def metric_fn(self, promp_true, promp_pred):\n","        \"\"\"Compute the RMSE value on a batch of data.\n","\n","        Args:\n","            promp_true: Ground truth tensor of the ProMP weights with shape (n_batch, n_dof*n_basis).\n","            promp_pred: Predicted tensor of the ProMP weights with shape (n_batch, n_dof*n_basis).\n","\n","        Returns:\n","            The MSE value on this batch.\n","        \"\"\"\n","\n","        # Compute trajectories from weights.\n","        traj_true = self.promp.trajectory_from_weights_tf(promp_true)\n","        traj_pred = self.promp.trajectory_from_weights_tf(promp_pred)\n","        # Convert to joint space.\n","        if self.promp_space == \"task\":\n","            # Convert from task to joint space.\n","            raise NotImplementedError(\"The method to convert from task to joint space has not been implemented yet!\")\n","        elif self.promp_space == \"joint\":\n","            # Trajectories are already in joint space.\n","            pass\n","        else:\n","            raise ValueError(f\"'{self.promp_space}' is not a valid value for 'promp_space'. It must be either 'joint' or 'task'.\")\n","        # Compute the RMSE on joint trajectories.\n","        rmse_joints = tf.sqrt(tf.reduce_mean(tf.square(traj_true - traj_pred), axis=1))\n","        # Average over joints and batch samples.\n","        rmse_joints = tf.reduce_mean(rmse_joints)\n","\n","        return rmse_joints\n"],"metadata":{"id":"8Sks8VGLLqFI","executionInfo":{"status":"ok","timestamp":1648120141258,"user_tz":0,"elapsed":6,"user":{"displayName":"Muhammad Arshad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvFtQPVWmKWzoK5CKIhL-D8VwVuWRpqJFIdPiupQ=s64","userId":"04461893466741361594"}}},"execution_count":28,"outputs":[]},{"cell_type":"code","source":["class MseJointsPromp(PrompTrajMeanMetric):\n","    \"\"\"MSE of the joint trajectories from a batch of ProMP weights of joint trajectories.\n","\n","    The value is averaged on all batches of each epoch.\n","    \"\"\"\n","\n","    def __init__(self, promp: ProMP, promp_space: str, name: str = \"mse_joints\", **kwargs):\n","        super().__init__(promp=promp, promp_space=promp_space, name=name, **kwargs)\n","\n","    def metric_fn(self, promp_true, promp_pred):\n","        \"\"\"Compute the MSE value on a batch of data.\n","\n","        Args:\n","            promp_true: Ground truth tensor of the ProMP weights with shape (n_batch, n_dof*n_basis).\n","            promp_pred: Predicted tensor of the ProMP weights with shape (n_batch, n_dof*n_basis).\n","\n","        Returns:\n","            The MSE value on this batch.\n","        \"\"\"\n","\n","        # Compute trajectories from weights.\n","        traj_true = self.promp.trajectory_from_weights_tf(promp_true)\n","        traj_pred = self.promp.trajectory_from_weights_tf(promp_pred)\n","        # Convert to joint space.\n","        if self.promp_space == \"task\":\n","            # Convert from task to joint space.\n","            raise NotImplementedError(\"The method to convert from task to joint space has not been implemented yet!\")\n","        elif self.promp_space == \"joint\":\n","            # Trajectories are already in joint space.\n","            pass\n","        else:\n","            raise ValueError(f\"'{self.promp_space}' is not a valid value for 'promp_space'. It must be either 'joint' or 'task'.\")\n","        # Compute the RMSE on joint trajectories.\n","        mse_joints = tf.reduce_mean(tf.square(traj_true - traj_pred), axis=1)\n","        # Average over joints and batch samples.\n","        mse_joints = tf.reduce_mean(mse_joints)\n","\n","        return mse_joints"],"metadata":{"id":"E93qEaSXLrFZ","executionInfo":{"status":"ok","timestamp":1648120141258,"user_tz":0,"elapsed":5,"user":{"displayName":"Muhammad Arshad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjvFtQPVWmKWzoK5CKIhL-D8VwVuWRpqJFIdPiupQ=s64","userId":"04461893466741361594"}}},"execution_count":29,"outputs":[]},{"cell_type":"code","source":["if __name__ == \"__main__\":\n","\n","  exp = RTP_RGBD()\n","  exp.train()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ajpam4EiV6F_","outputId":"d2142868-b2f4-47ed-ee62-2191bd5966cc"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["WARNING:tensorflow:SavedModel saved prior to TF 2.5 detected when loading Keras model. Please ensure that you are saving the model with model.save() or tf.keras.models.save_model(), *NOT* tf.saved_model.save(). To confirm, there should be a file named \"keras_metadata.pb\" in the SavedModel directory.\n","Model: \"RTP\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," encoded_image_input (InputL  [(None, 32, 32, 3)]      0         \n"," ayer)                                                           \n","                                                                 \n"," conv2d_4 (Conv2D)           (None, 32, 32, 32)        896       \n","                                                                 \n"," max_pooling2d_3 (MaxPooling  (None, 16, 16, 32)       0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_3 (Dropout)         (None, 16, 16, 32)        0         \n","                                                                 \n"," conv2d_5 (Conv2D)           (None, 16, 16, 16)        4624      \n","                                                                 \n"," max_pooling2d_4 (MaxPooling  (None, 8, 8, 16)         0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_4 (Dropout)         (None, 8, 8, 16)          0         \n","                                                                 \n"," conv2d_6 (Conv2D)           (None, 8, 8, 8)           1160      \n","                                                                 \n"," max_pooling2d_5 (MaxPooling  (None, 4, 4, 8)          0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_5 (Dropout)         (None, 4, 4, 8)           0         \n","                                                                 \n"," conv2d_7 (Conv2D)           (None, 4, 4, 4)           292       \n","                                                                 \n"," feature_vec (Flatten)       (None, 64)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 64)                4160      \n","                                                                 \n"," dense_3 (Dense)             (None, 70)                4550      \n","                                                                 \n","=================================================================\n","Total params: 15,682\n","Trainable params: 15,682\n","Non-trainable params: 0\n","_________________________________________________________________\n","Encoded images size:  (36, 32, 32, 3)\n"]}]}]}